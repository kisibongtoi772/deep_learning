{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train = X_train.reshape((-1,28*28)).T\n",
    "X_test = X_test.reshape((-1,28*28)).T\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_train= y_train.T\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_test= y_test.T\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters={}\n",
    "    L=len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)]= np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)]= np.zeros((layer_dims[l],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "def linear_forward(A,W,b):\n",
    "    #A là các activations (đầu vào) của lớp trước đó\n",
    "    #W là các trọng số (weights) của lớp hiện tại\n",
    "    #b là các bias của lớp hiện tại\n",
    "    Z=np.dot(W,A)+b\n",
    "    cache=(A,W,b)\n",
    "    return Z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A=1/(1+np.exp(-Z))\n",
    "    cache=Z\n",
    "    return A,cache\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A , cache\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def softmax(x):\n",
    "  e = np.exp(x)\n",
    "  return e / np.sum(e)\n",
    "def softmax_backward(dA,cache):\n",
    "    Z=cache\n",
    "    _s = cache.reshape(-1, 1)\n",
    "    dZ=np.multiply(np.diagonal(np.diagflat(_s) - np.dot(_s, _s.T)),dA)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "\n",
    "def linear_activation_forward(A_pre,W,b,activation):\n",
    "    if(activation=='sigmoid'):\n",
    "        Z,linear_cache = linear_forward(A_pre,W,b)\n",
    "        A,activation_cache=sigmoid(Z)\n",
    "    elif(activation=='relu'):\n",
    "        Z,linear_cache = linear_forward(A_pre,W,b)\n",
    "        A,activation_cache=relu(Z)\n",
    "    cache=(linear_cache,activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "def L_model_forward(X,paramters):\n",
    "    caches=[]\n",
    "    A=X\n",
    "    L=len(paramters)\n",
    "    for l in range(1,L):\n",
    "        A_pre=A\n",
    "        A, cache =linear_activation_forward(A_pre,paramters['W'+str(l)],paramters['b'+str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "    AL, cache= linear_activation_forward(A_pre,paramters['W'+str(L)],paramters[b+str(L)],'softmax')\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m=Y.shape[1]\n",
    "#     cost=-(1/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL)))\n",
    "    cost=-(1/m)*np.sum(np.sum(np.multiply(Y,np.log(AL))))\n",
    "    cost= np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "def linear_backward(dZ,cache):\n",
    "    A_pre,W,b = cache\n",
    "    m=A_pre.shape[1]\n",
    "    dW=(1/m)*np.dot(dZ,np.transpose(A_pre))\n",
    "    db=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_pre=np.dot(np.transpose(W),dZ)\n",
    "    return dA_pre,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "def linear_activation_backward(dA,cache,activation):\n",
    "    linear_cache,activation_cache= cache\n",
    "    if activation==\"relu\":\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "#         print(dZ.shape)\n",
    "        dA_pre,dW,db=linear_backward(dZ,linear_cache)\n",
    "    elif activation==\"sigmoid\":\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "#         print(dZ.shape)        \n",
    "        dA_pre,dW,db=linear_backward(dZ,linear_cache)\n",
    "    elif activation==\"softmax\":\n",
    "        dZ=softmax_backward(dA,activation_cache)\n",
    "#         print(dZ.shape)        \n",
    "        dA_pre,dW,db=linear_backward(dZ,linear_cache)\n",
    "    return dA_pre,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important\n",
    "def L_model_backward(AL,Y,caches):\n",
    "    grades={}\n",
    "    L= len(caches)\n",
    "    m= AL.shape[1]\n",
    "    Y=Y.reshape(AL.shape)\n",
    "    dAL=-(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "    current_cache=caches[L-1]\n",
    "#     grades['dA'+str(L)],grades['dW'+str(L)],grades['db'+str(L)]=linear_activation_backward(dAL,current_cache,'sigmoid')\n",
    "    grades['dA'+str(L)],grades['dW'+str(L)],grades['db'+str(L)]=linear_activation_backward(dAL,current_cache,'softmax')\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache=caches[l]\n",
    "        dA_pre_temp,dW_temp,db_temp= linear_activation_backward(grades['dA'+str(l+2)],current_cache,'relu')\n",
    "        grades['dA'+str(l+1)]=dA_pre_temp\n",
    "        grades['dW'+str(l+1)]=dW_temp\n",
    "        grades['db'+str(l+1)]=db_temp\n",
    "    return grades  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important\n",
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l+1)]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "    parameters = initialize_parameters_deep([n_x, n_h, n_y])\n",
    "\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SOFTMAX. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"softmax\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sofmax\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(X_train, y_train, layers_dims = (784, 16, 10), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
